{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "Q 02) Refer to the instructor's notebook on multi-colinearity. Use np.linalg.solve instead of np.linalg.inv for the same problem.\n",
        "Compare and contrast their usage, which one is better and why?"
      ],
      "metadata": {
        "id": "4OYhwO3GSZ5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Answer:-"
      ],
      "metadata": {
        "id": "HIIbGrbeScx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_normal_equation(X, y):\n",
        "    \"\"\"\n",
        "    This function Solves the normal equation for linear regression, handling potential singularity.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): The design matrix (independent variables).\n",
        "        y (np.ndarray): The dependent variable (target vector).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The estimated coefficients (theta) or None if the matrix is singular.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use np.linalg.solve instead of np.linalg.inv to handle singular matrices\n",
        "        theta = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "        return theta\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        print(f\"The matrix is singular: {e}\")\n",
        "        print(\"X.T @ X = \\n\", X.T @ X)\n",
        "        return None"
      ],
      "metadata": {
        "id": "ypJgjq94Sc9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Explanation:\n",
        "The provided code defines a function solve_normal_equation that addresses the issue of multicollinearity in linear regression\n",
        "caused by singular matrices.\n",
        "\n",
        "1. Purpose:\n",
        "This function estimates the coefficients (theta) for a linear regression model using the normal equation:\n",
        "theta = (X.T @ X)^(-1) @ X.T @ y\n",
        "where:\n",
        "x is the design matrix containing independent variables.\n",
        "y is the dependent variable (target vector).\n",
        "\n",
        "2. np.linalg.solve vs. np.linalg.inv:\n",
        "The original code used np.linalg.inv to calculate the inverse of (X.T @ X). However, this raises a LinAlgError if the matrix is singular\n",
        "(i.e., not invertible). This function replaces np.linalg.inv with np.linalg.solve.\n",
        "np.linalg.solve does not directly calculate the inverse. Instead, it efficiently solves the system of linear equations:\n",
        "(X.T @ X) * theta = X.T @ y\n",
        "\n",
        "This approach can handle singular matrices and provides an alternative solution, like a least-squares solution, even if an exact inverse doesn't exist.\n",
        "`np.linalg.solve(X.T @ X, X.T @ y)` directly solves the linear system of equations without explicitly calculating the inverse of the matrix `X.T @ X`.\n",
        "\n",
        "If the matrix `X.T @ X` is singular (non-invertible), it will raise a `LinAlgError` just like before, and the function will handle this by printing\n",
        "an error message.\n",
        "\n",
        "3. Error Handling:\n",
        "The function includes a try-except block to catch potential LinAlgError exceptions.\n",
        "If an error occurs, it prints an informative message and the singular matrix for debugging purposes.\n",
        "It returns None to indicate that the coefficients couldn't be estimated due to singularity.\n",
        "\n",
        "4. Advantages:\n",
        "Using np.linalg.solve enhances robustness by handling singular matrices, preventing the code from breaking if multicollinearity is present.\n",
        "It improves efficiency compared to calculating the inverse, especially for larger matrices.\n",
        "Additional Considerations:\n",
        "While np.linalg.solve addresses the singularity issue, it's crucial to investigate and address the underlying cause of multicollinearity\n",
        "for a reliable and interpretable model. Techniques like feature selection, regularization, and dimensionality reduction can be employed\n",
        "    depending on your specific scenario.\n",
        "Using `np.linalg.solve` is better than `np.linalg.inv` in this context because:\n",
        "It directly solves the linear system without explicitly calculating the inverse of the matrix, which can be numerically unstable and inefficient.\n",
        "`np.linalg.solve` is more robust and efficient for solving linear systems, especially when dealing with singular or ill-conditioned matrices.\n",
        "\n",
        "So, using `np.linalg.solve` enhances the stability and efficiency of the solution compared to using `np.linalg.inv`.\n"
      ],
      "metadata": {
        "id": "DwDUjNcBSdFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q 03) Referring to the same notebook, explain why Sklearn's linear regression implementation is robust against multicollinearity. Dive deep into\n",
        "Sklearn's code and explain in depth the methodology used in sklearn's implementation.\n"
      ],
      "metadata": {
        "id": "cokOUkkKSoqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Answer:-"
      ],
      "metadata": {
        "id": "NVTF9lS7So10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Scikit-learn's Linear Regression: Mitigating Multicollinearity\n",
        "\n",
        "1. Regularization (L2 Penalty):\n",
        "Scikit-learn's LinearRegression class applies L2 regularization by default with a coefficient of 1.0. This is implemented within\n",
        "the _solve_linear_system method, where a regularization term is added to the diagonal of the system matrix before solving.\n",
        "This penalty discourages large coefficients in the model, making it less susceptible to the influence of highly correlated features\n",
        "that might otherwise have inflated coefficients due to multicollinearity.\n",
        "\n",
        "2. Cholesky or QR Decomposition:\n",
        "Internally, Scikit-learn utilizes Cholesky decomposition (for smaller datasets) or QR decomposition (for larger datasets) to solve the system\n",
        "of equations for linear regression. These methods are employed within the _solve_linear_system method.\n",
        "These decomposition techniques are numerically stable and can handle near-collinearity better than directly inverting the design matrix.\n",
        "Direct inversion can be sensitive to multicollinearity and potentially lead to inaccurate or unstable solutions.\n",
        "\n",
        "3. Least Squares Optimization:\n",
        "Scikit-learn's LinearRegression class employs an iterative optimization algorithm (typically gradient descent) to find the coefficients that\n",
        "minimize the sum of squared errors (residuals). This is implemented within the _fit method.\n",
        "This indirect approach avoids direct manipulation of the potentially singular design matrix that can arise from multicollinearity.\n",
        "By minimizing the residuals, the model seeks to find a solution that fits the data well even in the presence of correlated features.\n",
        "Important Considerations:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RhR0o1ERSo9W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}